# -*- coding: utf-8 -*-
"""Promting

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10dP1H4BSYZRBbO1Sd7ipXtZ9ApJ6yjdH
"""

# Required Libraries
import pandas as pd
import os
import time
from openai import OpenAI

# =============== CONFIG ===============
# Read in GitHub Copilot token from file (as shown in the api_prompting.py example)

GITHUB_TOKEN = "InsertGithubToken"

os.environ["GITHUB_TOKEN"] = GITHUB_TOKEN

# Create OpenAI-compatible client using GitHub endpoint
client = OpenAI(
    base_url="https://models.inference.ai.azure.com",
    api_key=os.environ["GITHUB_TOKEN"]
)

# Model names
OPENAI_MODEL = "gpt-4o-mini"
LLAMA_MODEL = "Llama-4-Scout-17B-16E-Instruct"

# =============== FUNCTIONS ===============
def format_prompt(prompt, code):
    return f"{prompt}\n\n{code}"

def format_few_prompt(zero_prompt, few_prompt, code):
    return f"{zero_prompt}\n{few_prompt}\n{code}"

def query_model(prompt, model_name):
    try:
        print(f"Querying {model_name}...")
        messages = [{"role": "user", "content": prompt}]
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            max_tokens=1024,
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {e}"

# =============== MAIN PROCESS ===============
def run_prompt_evaluation(input_csv_path):
    df = pd.read_csv(input_csv_path)

    chatgpt_zero = []
    chatgpt_few = []
    llama_zero = []
    llama_few = []

    for idx, row in df.iterrows():
        code = row["Code"]
        zero_prompt = format_prompt(row["Zero_Shot_Prompt"], code)
        few_prompt = format_few_prompt(row["Zero_Shot_Prompt"], row["Few_Shot_Prompt"], code)

        # Query GitHub-hosted ChatGPT (gpt-4o-mini)
        chatgpt_zero.append(query_model(zero_prompt, OPENAI_MODEL))
        chatgpt_few.append(query_model(few_prompt, OPENAI_MODEL))

        # Query GitHub-hosted Llama (Gemini 2.0)
        llama_zero.append(query_model(zero_prompt, LLAMA_MODEL))
        llama_few.append(query_model(few_prompt, LLAMA_MODEL))

        time.sleep(1)  # Optional delay

    # Append results to new dataframes
    df_chatgpt = df.copy()
    df_chatgpt["Zero_Shot_Result - ChatGPT"] = chatgpt_zero
    df_chatgpt["Few_Shot_Result - ChatGPT"] = chatgpt_few

    df_llama = df.copy()
    df_llama["Zero_Shot_Result - Llama"] = llama_zero
    df_llama["Few_Shot_Result - Llama"] = llama_few

    # Save to CSV
    df_chatgpt.to_csv("chatgpt_results.csv", index=False)
    df_llama.to_csv("llama_results.csv", index=False)

    print("âœ… Evaluation complete. Results saved as chatgpt_results.csv and llama_results.csv")

# =============== USAGE EXAMPLE ===============
run_prompt_evaluation("/content/Corrected_CSV.csv")